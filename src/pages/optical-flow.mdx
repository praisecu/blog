---
layout: ../layouts/PostLayout.astro
title: "Motion Fields and Optical Flow: Geometry, Constraints, and Robotics Implications"
author: "Chahat Deep Singh"
description: "From 3D camera/object motion to 2D image motion: motion fields, optical flow, constraints, Lucas–Kanade, failure modes, and why the distinction matters in robotics."
date: "January 18, 2026"
image: "images/optical-flow.png"
tags: ["computer-vision", "optical-flow", "motion-fields", "geometry", "robotics"]
featured: false
---

import PyodideRunner from "../components/PyodideRunner.astro";

<PyodideRunner
  id="poly_reg_linear"
  title="Linear regression"
  height={380}
  autoRun={true}
  initialCode={`import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Data (same in every section)
rng = np.random.default_rng(0)
n = 220
x = np.sort(rng.uniform(0, 10, size=n))

y_true = 2.4 * x + 5.0 + 2.0 * np.sin(0.9 * x) + 0.12 * (x - 5.0) ** 2
y = y_true + rng.normal(0, 1.8, size=n)

def r2_score(y_true, y_pred):
    y_bar = np.mean(y_true)
    ss_tot = np.sum((y_true - y_bar) ** 2)
    ss_res = np.sum((y_true - y_pred) ** 2)
    return 1.0 - ss_res / ss_tot

# Closed-form LEast Square Solution:
x_mean = x.mean()
y_mean = y.mean()
b1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)
b0 = y_mean - b1 * x_mean

# Predictions
x_line = np.linspace(x.min(), x.max(), 600)
y_line = b0 + b1 * x_line
r2 = r2_score(y, b0 + b1 * x)

plt.rcParams.update({
    "figure.dpi": 140,
    "axes.spines.top": False,
    "axes.spines.right": False,
    "font.size": 11,
})

fig, ax = plt.subplots(1, 1, figsize=(8.4, 4.6), layout="constrained")
ax.scatter(x, y, s=34, alpha=0.78, edgecolors="white", linewidths=0.8, label="data")
ax.plot(x_line, y_line, linewidth=2.6, color="goldenrod", label="fit")
ax.set_title("Linear Regression")
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.grid(True, alpha=0.25)
ax.text(
    0.02, 0.98,
    f"$\\\\hat y = {b1:.3f}x + {b0:.3f}$\\n$R^2 = {r2:.3f}$",
    transform=ax.transAxes, va="top", ha="left",
    bbox=dict(boxstyle="round,pad=0.35", facecolor="white", alpha=0.92, linewidth=0.0),
)
ax.legend(frameon=False, loc="lower right")
plt.show()
`}
/>

## Learning Objectives

By the end of this lecture, you should be able to:

- Distinguish **motion fields** (true projected image motion induced by 3D motion) from **optical flow** (estimated image motion).
- Derive the **motion field equations** for camera translation and rotation.
- Explain why optical flow is **fundamentally ill-posed** without additional constraints.
- Understand the **brightness constancy constraint** and its consequences.
- Identify **failure modes** of optical flow that matter in robotics systems.
- Explain where **Lucas–Kanade** fits—and what it does *not* solve.

---

## 1. Image Motion Is Not a Primitive Quantity

In robotics and vision pipelines, “optical flow” is often treated as a primitive sensor measurement. This is incorrect.

There is no such thing as *true optical flow* in the physical world.

What exists physically is **3D motion**:
- of the camera,
- of objects,
- or both.

What we observe in images is the **projection of this 3D motion onto the image plane**.

This projection is called the **motion field**.

---

## 2. Motion Field: Definition

The **motion field** is the 2D velocity of image points induced by known 3D motion and scene geometry.

Formally:
- Let a 3D point $X(t)$ project to image coordinates $(x(t), y(t))$.
- The **motion field** is $(\dot{x}, \dot{y})$, obtained by differentiating the projection equations.

Crucially:
- The motion field **depends on depth**.
- It is **well-defined** even in textureless regions.
- It is **not directly observable** from image intensities alone.

---

## 3. Optical Flow: What It Actually Is

**Optical flow** is an *estimate* of image motion computed from brightness patterns over time.

It is:
- An *inference problem*, not a geometric one.
- Dependent on assumptions about the scene, lighting, and noise.
- Undefined where those assumptions fail.

The common but incorrect identification  
> *optical flow = image motion*  

is the root cause of many robotics failures.

---

## 4. Brightness Constancy Constraint

Most optical flow methods begin with the **brightness constancy assumption**:

$$
I(x, y, t) = I(x + \delta x, y + \delta y, t + \delta t)
$$

Linearizing yields the **optical flow constraint equation**:

$$
I_x u + I_y v + I_t = 0
$$

This is:
- One equation
- Two unknowns $(u, v)$

Therefore, **the problem is underdetermined**.

This is not a numerical issue.  
It is a **structural ambiguity**.

---

## 5. Aperture Problem (Not a Corner Case)

The aperture problem is not a pathological edge case—it is the *generic situation*.

- Along edges: motion perpendicular to the gradient is unobservable.
- In uniform regions: motion is completely unconstrained.

Any method that claims to “solve optical flow” without additional assumptions is mathematically incorrect.

---

## 6. Lucas–Kanade: What It Assumes

The **Lucas–Kanade** method resolves the ambiguity by assuming:

- Motion is **locally constant** within a small window.

This converts the underdetermined problem into a least-squares system.

Important consequences:
- It works **only where intensity gradients span multiple directions** (i.e., corners).
- It fails silently in low-texture regions.
- It does not recover the true motion field—only a *regularized estimate*.

Lucas–Kanade is an estimator, not a physical model.

---

## 7. Motion Field from Camera Motion (Key Result)

For a calibrated camera with focal length $f$, the motion field induced by:
- translational velocity $T = (T_x, T_y, T_z)$,
- rotational velocity $\omega = (\omega_x, \omega_y, \omega_z)$,

can be written as:

$$
u = \frac{-fT_x + xT_z}{Z} + xy\omega_x - (f^2 + x^2)\omega_y + y\omega_z
$$

$$
v = \frac{-fT_y + yT_z}{Z} + (f^2 + y^2)\omega_x - xy\omega_y - x\omega_z
$$

Key observations:
- **Translation depends on depth** $Z$.
- **Rotation does not**.
- Optical flow algorithms cannot separate these without additional information.

---

## 8. Why This Matters in Robotics

Confusing optical flow with motion fields leads to:

- Incorrect ego-motion estimates
- Spurious obstacle detection
- Control instability
- Dataset-specific “success” that collapses in deployment

In robotics:
- Optical flow is a *cue*, not a measurement.
- Geometry and dynamics must come first.

---

## 9. Summary (Non-Negotiable Facts)

- Motion fields are **physically defined**; optical flow is **inferred**.
- Optical flow is **ill-posed** without assumptions.
- Lucas–Kanade does not “solve” optical flow—it regularizes it.
- Depth is inseparable from translation-induced image motion.
- Treating optical flow as ground truth is a category error.

If you remember only one thing:

> **Optical flow is not motion. It is an estimate conditioned on assumptions.**

---
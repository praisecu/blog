---
layout: ../layouts/PostLayout.astro
title: "Motion Fields and Optical Flow"
author: "Chahat Deep Singh"
description: "From 3D camera/object motion to 2D image motion: motion fields, optical flow, constraints, Lucas–Kanade, failure modes, and why the distinction matters in robotics."
date: "January 18, 2026"
image: "images/optical-flow.png"
tags: ["computer-vision", "optical-flow", "motion-fields", "geometry", "robotics"]
featured: false
---

import PyodideRunner from "../components/PyodideRunner.astro";

## Feature detection (OpenCV.js)


<PyodideRunner
  id="poly_reg_linear"
  title="Harris Feature Detector"
  height={380}
  autoRun={true}
  initialCode={`
  
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import matplotlib.pyplot as plt
import io
from pyodide.http import pyfetch

URL = "/images/charles.png"

def conv2d(img, kernel):
    img = img.astype(np.float64)
    kH, kW = kernel.shape
    padH, padW = kH // 2, kW // 2
    padded = np.pad(img, ((padH, padH), (padW, padW)), mode="reflect")
    out = np.zeros_like(img, dtype=np.float64)
    ker = kernel[::-1, ::-1]
    H, W = img.shape
    for y in range(H):
        for x in range(W):
            patch = padded[y:y + kH, x:x + kW]
            out[y, x] = np.sum(patch * ker)
    return out

def gaussian_kernel(size=7, sigma=1.5):
    assert size % 2 == 1
    ax = np.arange(-(size // 2), size // 2 + 1)
    xx, yy = np.meshgrid(ax, ax)
    k = np.exp(-(xx**2 + yy**2) / (2 * sigma**2))
    return k / np.sum(k)

def nms_2d(R, radius=6, threshold_rel=0.02, max_points=250):
    R = R.copy()
    R[R < 0] = 0.0
    thr = threshold_rel * (R.max() + 1e-12)
    H, W = R.shape
    pts = []
    while True:
        idx = np.argmax(R)
        y, x = np.unravel_index(idx, R.shape)
        v = R[y, x]
        if v < thr:
            break
        pts.append((y, x, v))
        y0, y1 = max(0, y - radius), min(H, y + radius + 1)
        x0, x1 = max(0, x - radius), min(W, x + radius + 1)
        R[y0:y1, x0:x1] = 0.0
        if len(pts) >= max_points:
            break
    return pts

def harris_response(img_gray, k=0.04, win_size=7, win_sigma=1.5):
    sobel_x = np.array([[-1, 0, 1],
                        [-2, 0, 2],
                        [-1, 0, 1]], dtype=np.float64)
    sobel_y = np.array([[-1, -2, -1],
                        [ 0,  0,  0],
                        [ 1,  2,  1]], dtype=np.float64)

    Ix = conv2d(img_gray, sobel_x)
    Iy = conv2d(img_gray, sobel_y)

    Ixx, Iyy, Ixy = Ix * Ix, Iy * Iy, Ix * Iy

    G = gaussian_kernel(size=win_size, sigma=win_sigma)
    Sxx = conv2d(Ixx, G)
    Syy = conv2d(Iyy, G)
    Sxy = conv2d(Ixy, G)

    det = Sxx * Syy - Sxy * Sxy
    trace = Sxx + Syy
    return det - k * (trace * trace)

# Load image
resp = await pyfetch(URL)
data = await resp.bytes()
img = plt.imread(io.BytesIO(data))

# Prepare grayscale for detection
if img.ndim == 3:
    gray = img[..., :3].mean(axis=2)
else:
    gray = img.astype(np.float64)
gray = (gray - gray.min()) / (gray.max() - gray.min() + 1e-12)

# Harris + NMS
R = harris_response(gray, k=0.04, win_size=7, win_sigma=1.5)
pts = nms_2d(R, radius=6, threshold_rel=0.02, max_points=250)

ys = np.array([p[0] for p in pts], dtype=np.int32)
xs = np.array([p[1] for p in pts], dtype=np.int32)

# Display COLOR image (if RGB), overlay corners
plt.figure(figsize=(4.0, 4.0))
if img.ndim == 3:
    plt.imshow(img)            # color
else:
    plt.imshow(gray, cmap="gray")
plt.scatter(xs, ys, s=14, c="red")  # colored corners
plt.title(f"Harris corners: {len(pts)} points")
plt.axis("off")


`}
/>

## Learning Objectives

By the end of this lecture, you should be able to:

- Distinguish **motion fields** (true projected image motion induced by 3D motion) from **optical flow** (estimated image motion).
- Derive the **motion field equations** for camera translation and rotation.
- Explain why optical flow is **fundamentally ill-posed** without additional constraints.
- Understand the **brightness constancy constraint** and its consequences.
- Identify **failure modes** of optical flow that matter in robotics systems.
- Explain where **Lucas–Kanade** fits—and what it does *not* solve.

---

## 1. Image Motion Is Not a Primitive Quantity

In robotics and vision pipelines, “optical flow” is often treated as a primitive sensor measurement. This is incorrect.

There is no such thing as *true optical flow* in the physical world.

What exists physically is **3D motion**:
- of the camera,
- of objects,
- or both.

What we observe in images is the **projection of this 3D motion onto the image plane**.

This projection is called the **motion field**.

---

## 2. Motion Field: Definition

The **motion field** is the 2D velocity of image points induced by known 3D motion and scene geometry.

Formally:
- Let a 3D point $X(t)$ project to image coordinates $(x(t), y(t))$.
- The **motion field** is $(\dot{x}, \dot{y})$, obtained by differentiating the projection equations.

Crucially:
- The motion field **depends on depth**.
- It is **well-defined** even in textureless regions.
- It is **not directly observable** from image intensities alone.

---

## 3. Optical Flow: What It Actually Is

**Optical flow** is an *estimate* of image motion computed from brightness patterns over time.

It is:
- An *inference problem*, not a geometric one.
- Dependent on assumptions about the scene, lighting, and noise.
- Undefined where those assumptions fail.

The common but incorrect identification  
> *optical flow = image motion*  

is the root cause of many robotics failures.

---

## 4. Brightness Constancy Constraint

Most optical flow methods begin with the **brightness constancy assumption**:

$$
I(x, y, t) = I(x + \delta x, y + \delta y, t + \delta t)
$$

Linearizing yields the **optical flow constraint equation**:

$$
I_x u + I_y v + I_t = 0
$$

This is:
- One equation
- Two unknowns $(u, v)$

Therefore, **the problem is underdetermined**.

This is not a numerical issue.  
It is a **structural ambiguity**.

---

## 5. Aperture Problem (Not a Corner Case)

The aperture problem is not a pathological edge case—it is the *generic situation*.

- Along edges: motion perpendicular to the gradient is unobservable.
- In uniform regions: motion is completely unconstrained.

Any method that claims to “solve optical flow” without additional assumptions is mathematically incorrect.

---

## 6. Lucas–Kanade: What It Assumes

The **Lucas–Kanade** method resolves the ambiguity by assuming:

- Motion is **locally constant** within a small window.

This converts the underdetermined problem into a least-squares system.

Important consequences:
- It works **only where intensity gradients span multiple directions** (i.e., corners).
- It fails silently in low-texture regions.
- It does not recover the true motion field—only a *regularized estimate*.

Lucas–Kanade is an estimator, not a physical model.

---

## 7. Motion Field from Camera Motion (Key Result)

For a calibrated camera with focal length $f$, the motion field induced by:
- translational velocity $T = (T_x, T_y, T_z)$,
- rotational velocity $\omega = (\omega_x, \omega_y, \omega_z)$,

can be written as:

$$
u = \frac{-fT_x + xT_z}{Z} + xy\omega_x - (f^2 + x^2)\omega_y + y\omega_z
$$

$$
v = \frac{-fT_y + yT_z}{Z} + (f^2 + y^2)\omega_x - xy\omega_y - x\omega_z
$$

Key observations:
- **Translation depends on depth** $Z$.
- **Rotation does not**.
- Optical flow algorithms cannot separate these without additional information.

---

## 8. Why This Matters in Robotics

Confusing optical flow with motion fields leads to:

- Incorrect ego-motion estimates
- Spurious obstacle detection
- Control instability
- Dataset-specific “success” that collapses in deployment

In robotics:
- Optical flow is a *cue*, not a measurement.
- Geometry and dynamics must come first.

---

## 9. Summary (Non-Negotiable Facts)

- Motion fields are **physically defined**; optical flow is **inferred**.
- Optical flow is **ill-posed** without assumptions.
- Lucas–Kanade does not “solve” optical flow—it regularizes it.
- Depth is inseparable from translation-induced image motion.
- Treating optical flow as ground truth is a category error.

If you remember only one thing:

> **Optical flow is not motion. It is an estimate conditioned on assumptions.**

---
---
layout: ../layouts/PostLayout.astro
title: "Linear vs. Polynomial Regression in the Browser"
author: "Chahat Deep Singh"
description: "Fit and visualize linear, quadratic, cubic, and degree-5 polynomial regressions on mildly nonlinear data using Pyodide."
date: "January 18, 2026"
tags: ["machine-learning", "regression", "least-squares", "pyodide"]
featured: true
---

import PyodideRunner from "../components/PyodideRunner.astro";

# Linear vs. Nonlinear Regression

This example demonstrates how increasingly flexible polynomial models behave when the underlying data-generating process is **mildly nonlinear**. Instead of one stacked figure, we place a plot immediately after each model section:

1. Linear regression (ordinary least squares)
2. Quadratic regression (least squares)
3. Cubic regression (least squares)
4. Degree-5 polynomial regression (used here in place of 1D “bicubic”)

All models are fitted to the *same dataset* (same RNG seed and generation procedure). In every plot, the fitted curve is drawn in a **golden line** (Matplotlib color name `goldenrod`) to visually distinguish it from the scatter.

---

## 1. Data Generation

We generate one-dimensional input data $x \in [0, 10]$ and construct targets $y$ using a mildly nonlinear function:

$$
y = 2.4x + 5 + 2\sin(0.9x) + 0.12(x - 5)^2 + \varepsilon
$$

where $\varepsilon \sim \mathcal{N}(0, \sigma^2)$ is Gaussian noise.

Key points:
- The dominant trend is roughly linear.
- The sinusoidal and quadratic terms introduce systematic nonlinearity.
- Noise prevents perfect fitting by any deterministic model.

---

## 2. Linear Regression

The linear model is:

$$
\hat y = \beta_1 x + \beta_0
$$

Parameters are computed using the closed-form OLS solution:

$$
\beta_1 = \frac{\sum (x_i - \bar x)(y_i - \bar y)}{\sum (x_i - \bar x)^2},
\qquad
\beta_0 = \bar y - \beta_1 \bar x
$$

### What to expect
- Captures the average trend.
- Leaves structured residuals due to misspecification (nonlinearity).
- Typically exhibits lower $R^2$ than higher-order polynomials here.

<PyodideRunner
  id="poly_reg_linear"
  title="Linear regression (OLS)"
  height={380}
  autoRun={true}
  initialCode={`import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Data (same in every section)
rng = np.random.default_rng(0)
n = 220
x = np.sort(rng.uniform(0, 10, size=n))

y_true = 2.4 * x + 5.0 + 2.0 * np.sin(0.9 * x) + 0.12 * (x - 5.0) ** 2
y = y_true + rng.normal(0, 1.8, size=n)

def r2_score(y_true, y_pred):
    y_bar = np.mean(y_true)
    ss_tot = np.sum((y_true - y_bar) ** 2)
    ss_res = np.sum((y_true - y_pred) ** 2)
    return 1.0 - ss_res / ss_tot

# Closed-form OLS
x_mean = x.mean()
y_mean = y.mean()
b1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)
b0 = y_mean - b1 * x_mean

# Predictions
x_line = np.linspace(x.min(), x.max(), 600)
y_line = b0 + b1 * x_line
r2 = r2_score(y, b0 + b1 * x)

plt.rcParams.update({
    "figure.dpi": 140,
    "axes.spines.top": False,
    "axes.spines.right": False,
    "font.size": 11,
})

fig, ax = plt.subplots(1, 1, figsize=(8.4, 4.6), layout="constrained")
ax.scatter(x, y, s=34, alpha=0.78, edgecolors="white", linewidths=0.8, label="data")
ax.plot(x_line, y_line, linewidth=2.6, color="goldenrod", label="fit")
ax.set_title("Linear Regression (OLS)")
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.grid(True, alpha=0.25)
ax.text(
    0.02, 0.98,
    f"$\\\\hat y = {b1:.3f}x + {b0:.3f}$\\n$R^2 = {r2:.3f}$",
    transform=ax.transAxes, va="top", ha="left",
    bbox=dict(boxstyle="round,pad=0.35", facecolor="white", alpha=0.92, linewidth=0.0),
)
ax.legend(frameon=False, loc="lower right")
plt.show()
`}
/>

---

## 3. Quadratic Regression

A quadratic model introduces curvature:

$$
\hat y = a_2 x^2 + a_1 x + a_0
$$

Although nonlinear in $x$, it is linear in parameters $(a_2, a_1, a_0)$ and is solved by least squares:

$$
\min_a \|Xa - y\|_2^2
$$

with design matrix:

$$
X = \begin{bmatrix}
x_1^2 & x_1 & 1 \\
x_2^2 & x_2 & 1 \\
\vdots & \vdots & \vdots
\end{bmatrix}
$$

### What to expect
- Better captures curvature than the linear model.
- Higher $R^2$ in this dataset (on training data).

<PyodideRunner
  id="poly_reg_quadratic"
  title="Quadratic regression (least squares)"
  height={380}
  autoRun={true}
  initialCode={`import warnings
warnings.filterwarnings("ignore")

import numpy as np
import matplotlib.pyplot as plt

# Data (same in every section)
rng = np.random.default_rng(0)
n = 220
x = np.sort(rng.uniform(0, 10, size=n))

y_true = 2.4 * x + 5.0 + 2.0 * np.sin(0.9 * x) + 0.12 * (x - 5.0) ** 2
y = y_true + rng.normal(0, 1.8, size=n)

def r2_score(y_true, y_pred):
    y_bar = np.mean(y_true)
    ss_tot = np.sum((y_true - y_bar) ** 2)
    ss_res = np.sum((y_true - y_pred) ** 2)
    return 1.0 - ss_res / ss_tot

# Least squares quadratic
X2 = np.column_stack([x**2, x, np.ones_like(x)])
a2, a1, a0 = np.linalg.lstsq(X2, y, rcond=None)[0]

x_line = np.linspace(x.min(), x.max(), 600)
y_line = a2 * x_line**2 + a1 * x_line + a0
r2 = r2_score(y, a2 * x**2 + a1 * x + a0)

plt.rcParams.update({
    "figure.dpi": 140,
    "axes.spines.top": False,
    "axes.spines.right": False,
    "font.size": 11,
})

fig, ax = plt.subplots(1, 1, figsize=(8.4, 4.6), layout="constrained")
ax.scatter(x, y, s=34, alpha=0.78, edgecolors="white", linewidths=0.8, label="data")
ax.plot(x_line, y_line, linewidth=2.6, color="goldenrod", label="fit")
ax.set_title("Quadratic Regression (Least Squares)")
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.grid(True, alpha=0.25)
ax.text(
    0.02, 0.98,
    f"$\\\\hat y = {a2:.3f}x^2 + {a1:.3f}x + {a0:.3f}$\\n$R^2 = {r2:.3f}$",
    transform=ax.transAxes, va="top", ha="left",
    bbox=dict(boxstyle="round,pad=0.35", facecolor="white", alpha=0.92, linewidth=0.0),
)
ax.legend(frameon=False, loc="lower right")
plt.show()
`}
/>

---

## 4. Cubic Regression

The cubic model adds another degree of freedom:

$$
\hat y = c_3 x^3 + c_2 x^2 + c_1 x + c_0
$$

It is also solved by linear least squares using an appropriate polynomial design matrix.

### What to expect
- More flexible than quadratic.
- Can follow subtle structure better, but increased flexibility can also fit noise if pushed too far.

<PyodideRunner
  id="poly_reg_cubic"
  title="Cubic regression (least squares)"
  height={380}
  autoRun={true}
  initialCode={`import warnings
warnings.filterwarnings("ignore")

import numpy as np
import matplotlib.pyplot as plt

# Data (same in every section)
rng = np.random.default_rng(0)
n = 220
x = np.sort(rng.uniform(0, 10, size=n))

y_true = 2.4 * x + 5.0 + 2.0 * np.sin(0.9 * x) + 0.12 * (x - 5.0) ** 2
y = y_true + rng.normal(0, 1.8, size=n)

def r2_score(y_true, y_pred):
    y_bar = np.mean(y_true)
    ss_tot = np.sum((y_true - y_bar) ** 2)
    ss_res = np.sum((y_true - y_pred) ** 2)
    return 1.0 - ss_res / ss_tot

# Least squares cubic
X3 = np.column_stack([x**3, x**2, x, np.ones_like(x)])
c3, c2, c1, c0 = np.linalg.lstsq(X3, y, rcond=None)[0]

x_line = np.linspace(x.min(), x.max(), 600)
y_line = c3 * x_line**3 + c2 * x_line**2 + c1 * x_line + c0
r2 = r2_score(y, c3 * x**3 + c2 * x**2 + c1 * x + c0)

plt.rcParams.update({
    "figure.dpi": 140,
    "axes.spines.top": False,
    "axes.spines.right": False,
    "font.size": 11,
})

fig, ax = plt.subplots(1, 1, figsize=(8.4, 4.6), layout="constrained")
ax.scatter(x, y, s=34, alpha=0.78, edgecolors="white", linewidths=0.8, label="data")
ax.plot(x_line, y_line, linewidth=2.6, color="goldenrod", label="fit")
ax.set_title("Cubic Regression (Least Squares)")
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.grid(True, alpha=0.25)
ax.text(
    0.02, 0.98,
    f"$\\\\hat y = {c3:.3f}x^3 + {c2:.3f}x^2 + {c1:.3f}x + {c0:.3f}$\\n$R^2 = {r2:.3f}$",
    transform=ax.transAxes, va="top", ha="left",
    bbox=dict(boxstyle="round,pad=0.35", facecolor="white", alpha=0.92, linewidth=0.0),
)
ax.legend(frameon=False, loc="lower right")
plt.show()
`}
/>

---

## 5. Degree-5 Polynomial (Used Here Instead of 1D “Bicubic”)

Important clarification: **bicubic interpolation is a 2D gridded interpolation method** and is not a standard term for 1D polynomial regression. With a single scalar input $x$, there is no distinct notion of “bicubic regression.”

To satisfy the intent (a more flexible model than cubic) in a 1D setting, we use a degree-5 polynomial:

$$
\hat y = d_5 x^5 + d_4 x^4 + d_3 x^3 + d_2 x^2 + d_1 x + d_0
$$

### What to expect
- Higher flexibility and typically higher $R^2$ on training data.
- Increased risk of fitting noise rather than the underlying signal.
- This model is included to illustrate the bias–variance tradeoff.

If you actually have 2D inputs $(x, y)$ and want a true bicubic surface, you need a 2D gridded model/interpolant (or a 2D regression surface) rather than a 1D polynomial.

<PyodideRunner
  id="poly_reg_deg5"
  title="Degree-5 polynomial regression (least squares)"
  height={380}
  autoRun={true}
  initialCode={`import warnings
warnings.filterwarnings("ignore")

import numpy as np
import matplotlib.pyplot as plt

# Data (same in every section)
rng = np.random.default_rng(0)
n = 220
x = np.sort(rng.uniform(0, 10, size=n))

y_true = 2.4 * x + 5.0 + 2.0 * np.sin(0.9 * x) + 0.12 * (x - 5.0) ** 2
y = y_true + rng.normal(0, 1.8, size=n)

def r2_score(y_true, y_pred):
    y_bar = np.mean(y_true)
    ss_tot = np.sum((y_true - y_bar) ** 2)
    ss_res = np.sum((y_true - y_pred) ** 2)
    return 1.0 - ss_res / ss_tot

# Least squares degree-5
X5 = np.column_stack([x**5, x**4, x**3, x**2, x, np.ones_like(x)])
d5, d4, d3, d2, d1, d0 = np.linalg.lstsq(X5, y, rcond=None)[0]

x_line = np.linspace(x.min(), x.max(), 600)
y_line = d5 * x_line**5 + d4 * x_line**4 + d3 * x_line**3 + d2 * x_line**2 + d1 * x_line + d0
r2 = r2_score(y, d5 * x**5 + d4 * x**4 + d3 * x**3 + d2 * x**2 + d1 * x + d0)

plt.rcParams.update({
    "figure.dpi": 140,
    "axes.spines.top": False,
    "axes.spines.right": False,
    "font.size": 11,
})

fig, ax = plt.subplots(1, 1, figsize=(8.4, 4.6), layout="constrained")
ax.scatter(x, y, s=34, alpha=0.78, edgecolors="white", linewidths=0.8, label="data")
ax.plot(x_line, y_line, linewidth=2.6, color="goldenrod", label="fit")
ax.set_title("Degree-5 Polynomial Regression (Least Squares)")
ax.set_xlabel("x")
ax.set_ylabel("y")
ax.grid(True, alpha=0.25)
ax.text(
    0.02, 0.98,
    f"$\\\\hat y = {d5:.3f}x^5 + {d4:.3f}x^4 + {d3:.3f}x^3 + {d2:.3f}x^2 + {d1:.3f}x + {d0:.3f}$\\n$R^2 = {r2:.3f}$",
    transform=ax.transAxes, va="top", ha="left",
    bbox=dict(boxstyle="round,pad=0.35", facecolor="white", alpha=0.92, linewidth=0.0),
)
ax.legend(frameon=False, loc="lower right")
plt.show()
`}
/>

---

## 6. Evaluation Metric: $R^2$

Each plot reports **training** $R^2$ on the same data used to fit:

$$
R^2 = 1 - \frac{\sum (y_i - \hat y_i)^2}{\sum (y_i - \bar y)^2}
$$

Interpretation:
- Larger $R^2$ indicates the model explains more variance in the observed $y$.
- Training $R^2$ generally increases with model flexibility, but that does not guarantee better generalization.


<PyodideRunner
  id="poly_reg_l2_compare"
  title="Compare L2 error across models (same data)"
  height={340}
  autoRun={true}
  initialCode={`import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Data (must match the earlier sections exactly)
rng = np.random.default_rng(0)
n = 220
x = np.sort(rng.uniform(0, 10, size=n))

y_true = 2.4 * x + 5.0 + 2.0 * np.sin(0.9 * x) + 0.12 * (x - 5.0) ** 2
y = y_true + rng.normal(0, 1.8, size=n)

def l2_error(y_true, y_pred):
    # ||y - yhat||_2
    return float(np.linalg.norm(y_true - y_pred, ord=2))

def rmse(y_true, y_pred):
    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))

# ---- Fit models ----

# Linear (closed-form OLS)
x_mean = x.mean()
y_mean = y.mean()
b1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)
b0 = y_mean - b1 * x_mean
yhat_lin = b0 + b1 * x

# Quadratic (least squares)
X2 = np.column_stack([x**2, x, np.ones_like(x)])
a2, a1, a0 = np.linalg.lstsq(X2, y, rcond=None)[0]
yhat_quad = a2 * x**2 + a1 * x + a0

# Cubic (least squares)
X3 = np.column_stack([x**3, x**2, x, np.ones_like(x)])
c3, c2, c1, c0 = np.linalg.lstsq(X3, y, rcond=None)[0]
yhat_cub = c3 * x**3 + c2 * x**2 + c1 * x + c0

# Degree-5 (least squares)
X5 = np.column_stack([x**5, x**4, x**3, x**2, x, np.ones_like(x)])
d5, d4, d3, d2, d1, d0 = np.linalg.lstsq(X5, y, rcond=None)[0]
yhat_deg5 = d5 * x**5 + d4 * x**4 + d3 * x**3 + d2 * x**2 + d1 * x + d0

models = [
    ("Linear", yhat_lin),
    ("Quadratic", yhat_quad),
    ("Cubic", yhat_cub),
    ("Degree-5", yhat_deg5),
]

# ---- Compute errors ----
labels = []
l2s = []
rmses = []
for name, yhat in models:
    labels.append(name)
    l2s.append(l2_error(y, yhat))
    rmses.append(rmse(y, yhat))

# Print a clean numeric summary
print("Training-set errors (same data used to fit):")
for name, l2v, rv in zip(labels, l2s, rmses):
    print(f"{name:9s}  ||y - yhat||_2 = {l2v:8.3f}   RMSE = {rv:6.3f}")

# Plot: L2 error bars
# ---- Print summary (compact; avoids Pyodide stdout weirdness) ----
df = pd.DataFrame({
    "Model": labels,
    "L2 ||y-ŷ||2": l2s,
    "RMSE": rmses,
}).round({"L2 ||y-ŷ||2": 3, "RMSE": 3})
print(df.to_string(index=False))

# ---- Cleaner plot: RMSE + value labels ----
# ---- Modern thin bar plot: RMSE ----
plt.rcParams.update({
    "figure.dpi": 170,
    "font.size": 11,
})

fig, ax = plt.subplots(figsize=(7.4, 3.0), layout="constrained")

xpos = np.arange(len(labels))
bars = ax.bar(xpos, rmses, width=0.38)  # thinner bars

ax.set_title("Training RMSE across model classes", pad=10)
ax.set_ylabel("RMSE")
ax.set_xticks(xpos, labels)

# Minimal spines
for spine in ["top", "right", "left"]:
    ax.spines[spine].set_visible(False)

# Subtle y-grid only
ax.grid(True, axis="y", alpha=0.18, linewidth=0.8)
ax.tick_params(axis="y", length=0)

# Headroom + value labels
ymax = max(rmses)
ax.set_ylim(0, ymax * 1.22)

for b, v in zip(bars, rmses):
    ax.text(
        b.get_x() + b.get_width() / 2,
        b.get_height() + ymax * 0.03,
        f"{v:.2f}",
        ha="center",
        va="bottom",
        fontsize=10,
    )

plt.show()

`}
/>


---

## 7. Takeaways

- Linear regression underfits when $E[y|x]$ is nonlinear.
- Polynomial features reduce bias by expanding the model class.
- Higher-degree polynomials can overfit, especially when noise is nontrivial.
- “Bicubic” is not a meaningful label in 1D regression; it is fundamentally a 2D gridded interpolation concept.

---

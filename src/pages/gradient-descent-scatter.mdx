---
layout: ../layouts/PostLayout.astro
title: "Gradient Descent: An Interactive In-Browser Python Way"
author: "Chahat Deep Singh"
description: "A minimal Pyodide-powered code cell: generate and plot points. This validates the Python runner before adding gradient descent."
date: "January 18, 2026"
tags: ["optimization", "machine-learning", "gradient-descent", "pyodide"]
featured: false
---

import PyodideRunner from "../components/PyodideRunner.astro";

# From Linear to Polynomial Regression

This mini-demo shows three increasingly general ways to model a relationship between **x** and **y**:

1. **A line from two exact points** (deterministic geometry)
2. **Linear regression on noisy data** (best-fit line)
3. **Polynomial regression on noisy quadratic data** (best-fit curve), comparing **degree 2** (correct model class) vs **degree 6** (more flexible)

---

## 1) Line through two points (no learning)

Given two points $(x_1, y_1)$ and $(x_2, y_2)$, the unique line (if $x_1 \neq x_2$) is:

$m = \frac{y_2 - y_1}{x_2 - x_1}$  
$b = y_1 - mx_1$  
$y = mx + b$

This is *not* regression—there is no notion of noise or “best fit.” The line is fully determined.

**Important edge case:** if $x_1 = x_2$, the line is vertical and cannot be written as $y = mx + b$.

---

## 2) Linear regression (degree 1) on 100 noisy samples

Now we assume the data is noisy, e.g.

$y = 3x - 1 + \epsilon$, with $\epsilon \sim \mathcal{N}(0,\sigma^2)$

A line won’t pass through all points (because of noise), so we learn parameters $\theta = [b, m]^T$ that minimize squared error:

$\min_{\theta} \|X\theta - y\|_2^2$

with the design matrix:

$X =
\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_N
\end{bmatrix}$

This is the classic least-squares solution (computed via `np.linalg.lstsq`).

---

## 3) Quadratic regression (degree 2) on quadratic data

Next we generate data from a quadratic ground truth:

$y = ax^2 + bx + c + \epsilon$

Polynomial regression of degree 2 fits:

$y \approx c + bx + ax^2$

with design matrix:

$X =
\begin{bmatrix}
1 & x_1 & x_1^2 \\
1 & x_2 & x_2^2 \\
\vdots & \vdots & \vdots \\
1 & x_N & x_N^2
\end{bmatrix}$

This model class matches the data-generating process, so it typically fits well and generalizes well.

---




<PyodideRunner
  id="gd_scatter_demo"
  title="Generate + plot synthetic points"
  height={240}
  autoRun={true}
  initialCode={`import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ---------------------------
# data (nonlinear ground truth)
# ---------------------------
rng = np.random.default_rng(0)
n = 180

df = pd.DataFrame({"x": np.sort(rng.uniform(0, 10, size=n))})
x = df["x"].to_numpy()

y_true = (
    2.4 * x
    + 5.0
    + 2.0 * np.sin(0.9 * x)
    + 0.12 * (x - 5.0) ** 2
)
y = y_true + rng.normal(0, 1.8, size=n)
df["y"] = y

# ---------------------------
# linear regression (OLS)
# ---------------------------
x_mean = x.mean()
y_mean = y.mean()
b1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)
b0 = y_mean - b1 * x_mean

# ---------------------------
# nonlinear regression (quadratic: y = a2 x^2 + a1 x + a0)
# (still linear in parameters; solved by least squares)
# ---------------------------
X_poly = np.column_stack([x**2, x, np.ones_like(x)])
a2, a1, a0 = np.linalg.lstsq(X_poly, y, rcond=None)[0]

# prediction grid
x_line = np.linspace(x.min(), x.max(), 500)
y_lin = b0 + b1 * x_line
y_quad = a2 * x_line**2 + a1 * x_line + a0

# R^2
y_hat_lin = b0 + b1 * x
y_hat_quad = a2 * x**2 + a1 * x + a0
ss_tot = np.sum((y - y_mean) ** 2)
r2_lin = 1.0 - np.sum((y - y_hat_lin) ** 2) / ss_tot
r2_quad = 1.0 - np.sum((y - y_hat_quad) ** 2) / ss_tot

# ---------------------------
# plotting
# ---------------------------
plt.rcParams.update({
    "figure.dpi": 140,
    "axes.spines.top": False,
    "axes.spines.right": False,
    "font.size": 11,
})

fig, ax = plt.subplots(2, 1, figsize=(8.2, 8.0), sharex=True, layout="constrained")

# Top: linear
ax[0].scatter(x, y, s=38, alpha=0.78, edgecolors="white", linewidths=0.8, label="data")
ax[0].plot(x_line, y_lin, linewidth=2.6, label="linear fit")
ax[0].set_title("Linear Regression (OLS)")
ax[0].set_ylabel("y")
ax[0].grid(True, alpha=0.25)
ax[0].text(
    0.02, 0.98,
    f"$\\hat y = {b1:.3f}x + {b0:.3f}$\n$R^2 = {r2_lin:.3f}$",
    transform=ax[0].transAxes,
    va="top", ha="left",
    bbox=dict(boxstyle="round,pad=0.35", facecolor="white", alpha=0.92, linewidth=0.0),
)
ax[0].legend(frameon=False, loc="lower right")

# Bottom: nonlinear (quadratic)
ax[1].scatter(x, y, s=38, alpha=0.78, edgecolors="white", linewidths=0.8, label="data")
ax[1].plot(x_line, y_quad, linewidth=2.6, label="quadratic fit")
ax[1].set_title("Nonlinear Regression (Quadratic Least Squares)")
ax[1].set_xlabel("x")
ax[1].set_ylabel("y")
ax[1].grid(True, alpha=0.25)
ax[1].text(
    0.02, 0.98,
    f"$\\hat y = {a2:.3f}x^2 + {a1:.3f}x + {a0:.3f}$\n$R^2 = {r2_quad:.3f}$",
    transform=ax[1].transAxes,
    va="top", ha="left",
    bbox=dict(boxstyle="round,pad=0.35", facecolor="white", alpha=0.92, linewidth=0.0),
)
ax[1].legend(frameon=False, loc="lower right")

plt.show()
`}
/>

---





<!-- import numpy as np
import matplotlib.pyplot as plt

np.random.seed(0)

# -------------------------
# 1) Line through two points
# -------------------------
x1, y1 = 1.0, 2.0
x2, y2 = 4.0, 5.5
if x2 == x1:
    raise ValueError("Vertical line: x2 == x1, cannot represent as y = m x + b.")
m = (y2 - y1) / (x2 - x1)
b = y1 - m * x1
xs = np.linspace(0, 6, 200)
ys = m * xs + b

# -------------------------
# 2) Linear data + linear regression fit
# -------------------------
N = 100
x = np.random.uniform(-2, 2, size=N)
true_m, true_b = 3.0, -1.0
sigma = 0.6
y = true_m * x + true_b + np.random.normal(0, sigma, size=N)

X_lin = np.column_stack([np.ones_like(x), x])
theta_lin, *_ = np.linalg.lstsq(X_lin, y, rcond=None)
b_hat, m_hat = theta_lin

xg = np.linspace(x.min(), x.max(), 300)
y_true_lin = true_b + true_m * xg
y_fit_lin = b_hat + m_hat * xg

# -------------------------
# 3) Quadratic data + degree-2 polynomial regression fit (quadratic)
# -------------------------
xq = np.random.uniform(-2, 2, size=N)
true_a, true_b2, true_c = 1.5, -0.3, 2.0
yq = true_a * xq**2 + true_b2 * xq + true_c + np.random.normal(0, sigma, size=N)

# Degree-2 fit: [1, x, x^2]
X_deg2 = np.column_stack([np.ones_like(xq), xq, xq**2])
theta_deg2, *_ = np.linalg.lstsq(X_deg2, yq, rcond=None)
c2_hat, b2_hat, a2_hat = theta_deg2

xgq = np.linspace(xq.min(), xq.max(), 400)
y_true_quad = true_a * xgq**2 + true_b2 * xgq + true_c
y_fit_deg2 = a2_hat * xgq**2 + b2_hat * xgq + c2_hat

# -------------------------
# 4) Same quadratic data + degree-6 polynomial regression fit
# -------------------------
# Degree-6 fit: [1, x, x^2, ..., x^6]
X_deg6 = np.column_stack([xq**k for k in range(7)])
theta_deg6, *_ = np.linalg.lstsq(X_deg6, yq, rcond=None)

Xg_deg6 = np.column_stack([xgq**k for k in range(7)])
y_fit_deg6 = Xg_deg6 @ theta_deg6

# -------------------------
# Single figure with 4 subplots
# -------------------------
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# Subplot 1: line through two points
ax = axes[0, 0]
ax.scatter([x1, x2], [y1, y2], label="points")
ax.plot(xs, ys, label="line")
ax.set_title("1) Line through two points")
ax.set_xlabel("x"); ax.set_ylabel("y")
ax.legend()

# Subplot 2: linear regression
ax = axes[0, 1]
ax.scatter(x, y, s=20, label="data")
ax.plot(xg, y_fit_lin, label="fit")
ax.plot(xg, y_true_lin, linestyle="--", label="true")
ax.set_title("2) Linear regression (deg 1)")
ax.set_xlabel("x"); ax.set_ylabel("y")
ax.legend()

# Subplot 3: quadratic regression (degree 2)
ax = axes[1, 0]
ax.scatter(xq, yq, s=20, label="data")
ax.plot(xgq, y_fit_deg2, label="fit (deg 2)")
ax.plot(xgq, y_true_quad, linestyle="--", label="true")
ax.set_title("3) Polynomial regression (degree 2)")
ax.set_xlabel("x"); ax.set_ylabel("y")
ax.legend()

# Subplot 4: degree-6 regression on the same quadratic data
ax = axes[1, 1]
ax.scatter(xq, yq, s=20, label="data")
ax.plot(xgq, y_fit_deg6, label="fit (deg 6)")
ax.plot(xgq, y_true_quad, linestyle="--", label="true (quadratic)")
ax.set_title("4) Polynomial regression (degree 6)")
ax.set_xlabel("x"); ax.set_ylabel("y")
ax.legend()

plt.tight_layout()
plt.show() -->
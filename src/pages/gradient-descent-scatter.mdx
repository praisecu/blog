---
layout: ../layouts/PostLayout.astro
title: "Gradient Descent: An Interactive In-Browser Python Way"
author: "Chahat Deep Singh"
description: "A minimal Pyodide-powered code cell: generate and plot points. This validates the Python runner before adding gradient descent."
date: "January 18, 2026"
tags: ["optimization", "machine-learning", "gradient-descent", "pyodide"]
featured: true
---

import PyodideRunner from "../components/PyodideRunner.astro";

# Linear vs. Nonlinear Regression (Illustrative Example)

This example demonstrates how a **linear regression model** behaves when the underlying data-generating process is **slightly nonlinear**, and how a simple **nonlinear (quadratic) regression** can better capture that structure.

The visualization consists of **two subplots**:
- **Top:** Linear regression (ordinary least squares)
- **Bottom:** Nonlinear regression (quadratic least squares)

Both models are fitted to the *same data*.

---

## 1. Data Generation

We generate one-dimensional input data $x \in [0, 10]$ and construct targets $y$ using a mildly nonlinear function:

$$
y = 2.4x + 5 + 2\sin(0.9x) + 0.12(x - 5)^2 + \varepsilon
$$

where $\varepsilon \sim \mathcal{N}(0, \sigma^2)$ is Gaussian noise.

Key points:
- The dominant trend is approximately linear.
- The sinusoidal and quadratic terms introduce systematic nonlinearity.
- Noise prevents perfect fitting by any deterministic model.

---

## 2. Linear Regression (Top Plot)

The linear model assumes:

$$
\hat y = \beta_1 x + \beta_0
$$

The parameters $\beta_1$ and $\beta_0$ are computed using the **closed-form ordinary least squares (OLS)** solution:

$$
\beta_1 = \frac{\sum (x_i - \bar x)(y_i - \bar y)}{\sum (x_i - \bar x)^2},
\qquad
\beta_0 = \bar y - \beta_1 \bar x
$$

### Interpretation

- The model captures the *average trend* in the data.
- Systematic deviations remain because the true relationship is nonlinear.
- This manifests visually as structured residuals.
- The coefficient of determination $R^2$ is limited by model bias.

This plot illustrates **underfitting**.

---

## 3. Nonlinear Regression (Bottom Plot)

To better model the curvature, we use a quadratic function:

$$
\hat y = a_2 x^2 + a_1 x + a_0
$$

Although this is a *nonlinear function of $x$*, it is still **linear in the parameters**, allowing solution via least squares:

$$
\min_{a_0,a_1,a_2} \| Xa - y \|_2^2
$$

where:

$$
X = \begin{bmatrix}
x_1^2 & x_1 & 1 \\
x_2^2 & x_2 & 1 \\
\vdots & \vdots & \vdots
\end{bmatrix}
$$

### Interpretation

- The quadratic term absorbs curvature in the data.
- Residual structure is significantly reduced.
- $R^2$ increases, reflecting improved explanatory power.
- The model still remains simple and interpretable.

This plot demonstrates **reduced bias** with modest complexity.

---

## 4. Comparison and Takeaways

| Aspect | Linear Model | Quadratic Model |
|------|-------------|----------------|
| Assumed relationship | $y = \beta_1 x + \beta_0$ | $y = a_2 x^2 + a_1 x + a_0$ |
| Model bias | High | Lower |
| Variance | Low | Slightly higher |
| Interpretability | Very high | High |
| Fit to nonlinear data | Poor | Better |

Key conclusions:
- Linear regression fails when the conditional mean $E[y|x]$ is nonlinear.
- Adding polynomial terms is a principled way to reduce bias.
- This example illustrates the **biasâ€“variance tradeoff** in a controlled setting.

---

## 5. Why This Matters

In practice:
- Linear models are often used as baselines.
- Residual plots or systematic deviations signal model misspecification.
- Polynomial or feature-based nonlinear models can often fix this without resorting to complex methods.

This example shows *why visual inspection and model assumptions matter*, even for simple regression problems.


---




<PyodideRunner
  id="gd_scatter_demo"
  title="Generate + plot synthetic points"
  height={240}
  autoRun={true}
  initialCode={`import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ---------------------------
# data (nonlinear ground truth)
# ---------------------------
rng = np.random.default_rng(0)
n = 180

df = pd.DataFrame({"x": np.sort(rng.uniform(0, 10, size=n))})
x = df["x"].to_numpy()

y_true = (
    2.4 * x
    + 5.0
    + 2.0 * np.sin(0.9 * x)
    + 0.12 * (x - 5.0) ** 2
)
y = y_true + rng.normal(0, 1.8, size=n)
df["y"] = y

# ---------------------------
# linear regression (OLS)
# ---------------------------
x_mean = x.mean()
y_mean = y.mean()
b1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)
b0 = y_mean - b1 * x_mean

# ---------------------------
# nonlinear regression (quadratic: y = a2 x^2 + a1 x + a0)
# (still linear in parameters; solved by least squares)
# ---------------------------
X_poly = np.column_stack([x**2, x, np.ones_like(x)])
a2, a1, a0 = np.linalg.lstsq(X_poly, y, rcond=None)[0]

# prediction grid
x_line = np.linspace(x.min(), x.max(), 500)
y_lin = b0 + b1 * x_line
y_quad = a2 * x_line**2 + a1 * x_line + a0

# R^2
y_hat_lin = b0 + b1 * x
y_hat_quad = a2 * x**2 + a1 * x + a0
ss_tot = np.sum((y - y_mean) ** 2)
r2_lin = 1.0 - np.sum((y - y_hat_lin) ** 2) / ss_tot
r2_quad = 1.0 - np.sum((y - y_hat_quad) ** 2) / ss_tot

# ---------------------------
# plotting
# ---------------------------
plt.rcParams.update({
    "figure.dpi": 140,
    "axes.spines.top": False,
    "axes.spines.right": False,
    "font.size": 11,
})

fig, ax = plt.subplots(2, 1, figsize=(8.2, 8.0), sharex=True, layout="constrained")

# Top: linear
ax[0].scatter(x, y, s=38, alpha=0.78, edgecolors="white", linewidths=0.8, label="data")
ax[0].plot(x_line, y_lin, linewidth=2.6, label="linear fit")
ax[0].set_title("Linear Regression (OLS)")
ax[0].set_ylabel("y")
ax[0].grid(True, alpha=0.25)
ax[0].text(
    0.02, 0.98,
    f"$\\hat y = {b1:.3f}x + {b0:.3f}$\n$R^2 = {r2_lin:.3f}$",
    transform=ax[0].transAxes,
    va="top", ha="left",
    bbox=dict(boxstyle="round,pad=0.35", facecolor="white", alpha=0.92, linewidth=0.0),
)
ax[0].legend(frameon=False, loc="lower right")

# Bottom: nonlinear (quadratic)
ax[1].scatter(x, y, s=38, alpha=0.78, edgecolors="white", linewidths=0.8, label="data")
ax[1].plot(x_line, y_quad, linewidth=2.6, label="quadratic fit")
ax[1].set_title("Nonlinear Regression (Quadratic Least Squares)")
ax[1].set_xlabel("x")
ax[1].set_ylabel("y")
ax[1].grid(True, alpha=0.25)
ax[1].text(
    0.02, 0.98,
    f"$\\hat y = {a2:.3f}x^2 + {a1:.3f}x + {a0:.3f}$\n$R^2 = {r2_quad:.3f}$",
    transform=ax[1].transAxes,
    va="top", ha="left",
    bbox=dict(boxstyle="round,pad=0.35", facecolor="white", alpha=0.92, linewidth=0.0),
)
ax[1].legend(frameon=False, loc="lower right")

plt.show()
`}
/>

---

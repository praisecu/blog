---
layout: ../layouts/PostLayout.astro
title: "Linear vs. Polynomial Regression in the Browser (Pyodide)"
author: "Chahat Deep Singh"
description: "Fit and visualize linear, quadratic, cubic, and degree-5 polynomial regressions on mildly nonlinear data using Pyodide."
date: "January 18, 2026"
tags: ["machine-learning", "regression", "least-squares", "pyodide"]
featured: true
---

import PyodideRunner from "../components/PyodideRunner.astro";

# Linear vs. Nonlinear Regression

This example demonstrates how increasingly flexible polynomial models behave when the underlying data-generating process is **mildly nonlinear**. The visualization consists of **four vertically stacked subplots**:

1. Linear regression (ordinary least squares)
2. Quadratic regression (least squares)
3. Cubic regression (least squares)
4. Degree-5 polynomial regression (used here in place of 1D “bicubic”)

All models are fitted to the *same dataset*. In every subplot, the fitted curve is drawn in a **golden line** (Matplotlib color name `goldenrod`) to visually distinguish it from the scatter.

---

## 1. Data Generation

We generate one-dimensional input data $x \in [0, 10]$ and construct targets $y$ using a mildly nonlinear function:

$$
y = 2.4x + 5 + 2\sin(0.9x) + 0.12(x - 5)^2 + \varepsilon
$$

where $\varepsilon \sim \mathcal{N}(0, \sigma^2)$ is Gaussian noise.

Key points:
- The dominant trend is roughly linear.
- The sinusoidal and quadratic terms introduce systematic nonlinearity.
- Noise prevents perfect fitting by any deterministic model.

---

## 2. Linear Regression

The linear model is:

$$
\hat y = \beta_1 x + \beta_0
$$

Parameters are computed using the closed-form OLS solution:

$$
\beta_1 = \frac{\sum (x_i - \bar x)(y_i - \bar y)}{\sum (x_i - \bar x)^2},
\qquad
\beta_0 = \bar y - \beta_1 \bar x
$$

### What to expect
- Captures the average trend.
- Leaves structured residuals due to misspecification (nonlinearity).
- Typically exhibits lower $R^2$ than higher-order polynomials here.

---

## 3. Quadratic Regression

A quadratic model introduces curvature:

$$
\hat y = a_2 x^2 + a_1 x + a_0
$$

Although nonlinear in $x$, it is linear in parameters $(a_2, a_1, a_0)$ and is solved by least squares:

$$
\min_a \|Xa - y\|_2^2
$$

with design matrix:

$$
X = \begin{bmatrix}
x_1^2 & x_1 & 1 \\
x_2^2 & x_2 & 1 \\
\vdots & \vdots & \vdots
\end{bmatrix}
$$

### What to expect
- Better captures curvature than the linear model.
- Higher $R^2$ in this dataset.

---

## 4. Cubic Regression

The cubic model adds another degree of freedom:

$$
\hat y = c_3 x^3 + c_2 x^2 + c_1 x + c_0
$$

It is also solved by linear least squares using an appropriate polynomial design matrix.

### What to expect
- More flexible than quadratic.
- Can follow subtle structure better, but increased flexibility can also fit noise if pushed too far.

---

## 5. Degree-5 Polynomial (Used Here Instead of 1D “Bicubic”)

Important clarification: **“bicubic” is a 2D interpolation concept**, typically for mapping a grid of values $(x, y) \mapsto z$ using a bicubic surface. With a single scalar input $x$, there is no distinct notion of “bicubic regression.”

To satisfy the intent (a more flexible model than cubic) in a 1D setting, we use a degree-5 polynomial:

$$
\hat y = d_5 x^5 + d_4 x^4 + d_3 x^3 + d_2 x^2 + d_1 x + d_0
$$

### What to expect
- Higher flexibility and potentially higher $R^2$ on training data.
- Increased risk of fitting noise rather than the underlying signal.
- This model is included to illustrate the bias–variance tradeoff.

If you actually have 2D inputs $(x, y)$ and want a true bicubic surface model $z=f(x,y)$, the model must be changed accordingly.

---

## 6. Evaluation Metric: $R^2$

Each subplot reports $R^2$:

$$
R^2 = 1 - \frac{\sum (y_i - \hat y_i)^2}{\sum (y_i - \bar y)^2}
$$

Interpretation:
- Larger $R^2$ indicates the model explains more variance in the observed $y$.
- Training $R^2$ generally increases with model flexibility, but that does not guarantee better generalization.

---

## 7. Takeaways

- Linear regression underfits when $E[y|x]$ is nonlinear.
- Polynomial features reduce bias by expanding the model class.
- Higher-degree polynomials can overfit, especially when noise is nontrivial.
- “Bicubic” is not a meaningful label in 1D regression; it is fundamentally a 2D interpolation/surface-fitting concept.

---

<PyodideRunner
  id="poly_reg_demo"
  title="Linear vs polynomial regression (4 fits)"
  height={480}
  autoRun={true}
  initialCode={`import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

rng = np.random.default_rng(0)
n = 220

df = pd.DataFrame({"x": np.sort(rng.uniform(0, 10, size=n))})
x = df["x"].to_numpy()

y_true = 2.4 * x + 5.0 + 2.0 * np.sin(0.9 * x) + 0.12 * (x - 5.0) ** 2
y = y_true + rng.normal(0, 1.8, size=n)
df["y"] = y

x_mean = x.mean()
y_mean = y.mean()
ss_tot = np.sum((y - y_mean) ** 2)

def r2_score(y_true, y_pred):
    return 1.0 - np.sum((y_true - y_pred) ** 2) / ss_tot

# Linear
b1 = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)
b0 = y_mean - b1 * x_mean

# Quadratic
X2 = np.column_stack([x**2, x, np.ones_like(x)])
a2, a1, a0 = np.linalg.lstsq(X2, y, rcond=None)[0]

# Cubic
X3 = np.column_stack([x**3, x**2, x, np.ones_like(x)])
c3, c2, c1, c0 = np.linalg.lstsq(X3, y, rcond=None)[0]

# Degree-5 (stand-in for 1D "bicubic")
X5 = np.column_stack([x**5, x**4, x**3, x**2, x, np.ones_like(x)])
d5, d4, d3, d2, d1, d0 = np.linalg.lstsq(X5, y, rcond=None)[0]

x_line = np.linspace(x.min(), x.max(), 600)
y_lin  = b0 + b1 * x_line
y_quad = a2 * x_line**2 + a1 * x_line + a0
y_cub  = c3 * x_line**3 + c2 * x_line**2 + c1 * x_line + c0
y_deg5 = d5 * x_line**5 + d4 * x_line**4 + d3 * x_line**3 + d2 * x_line**2 + d1 * x_line + d0

r2_lin  = r2_score(y, b0 + b1 * x)
r2_quad = r2_score(y, a2 * x**2 + a1 * x + a0)
r2_cub  = r2_score(y, c3 * x**3 + c2 * x**2 + c1 * x + c0)
r2_deg5 = r2_score(y, d5 * x**5 + d4 * x**4 + d3 * x**3 + d2 * x**2 + d1 * x + d0)

plt.rcParams.update({
    "figure.dpi": 140,
    "axes.spines.top": False,
    "axes.spines.right": False,
    "font.size": 11,
})

fig, ax = plt.subplots(4, 1, figsize=(8.4, 13.0), sharex=True, layout="constrained")

def draw_panel(ax_i, title, y_fit_line, eq_text):
    ax_i.scatter(x, y, s=34, alpha=0.78, edgecolors="white", linewidths=0.8, label="data")
    ax_i.plot(x_line, y_fit_line, linewidth=2.6, color="goldenrod", label="fit")
    ax_i.set_title(title)
    ax_i.set_ylabel("y")
    ax_i.grid(True, alpha=0.25)
    ax_i.text(
        0.02, 0.98, eq_text,
        transform=ax_i.transAxes,
        va="top", ha="left",
        bbox=dict(boxstyle="round,pad=0.35", facecolor="white", alpha=0.92, linewidth=0.0),
    )
    ax_i.legend(frameon=False, loc="lower right")

draw_panel(
    ax[0],
    "Linear Regression (OLS)",
    y_lin,
    f"$\\hat y = {b1:.3f}x + {b0:.3f}$\\n$R^2 = {r2_lin:.3f}$",
)

draw_panel(
    ax[1],
    "Quadratic Regression (Least Squares)",
    y_quad,
    f"$\\hat y = {a2:.3f}x^2 + {a1:.3f}x + {a0:.3f}$\\n$R^2 = {r2_quad:.3f}$",
)

draw_panel(
    ax[2],
    "Cubic Regression (Least Squares)",
    y_cub,
    f"$\\hat y = {c3:.3f}x^3 + {c2:.3f}x^2 + {c1:.3f}x + {c0:.3f}$\\n$R^2 = {r2_cub:.3f}$",
)

draw_panel(
    ax[3],
    "Degree-5 Polynomial (used here in place of 1D 'bicubic')",
    y_deg5,
    f"$\\hat y = {d5:.3f}x^5 + {d4:.3f}x^4 + {d3:.3f}x^3 + {d2:.3f}x^2 + {d1:.3f}x + {d0:.3f}$\\n$R^2 = {r2_deg5:.3f}$",
)

ax[3].set_xlabel("x")
plt.show()
`}
/>

---

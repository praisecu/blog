---
layout: ../../layouts/PostLayout.astro
title: "Optical Flow vs Motion Flow: Geometry, Failure Modes, and Robotics Implications"
author: "Chahat"
date: "January 18, 2026"
---

## Introduction

“Optical flow” is often used as shorthand for “image motion,” but that conflation is exactly what causes confusion in robotics pipelines.
Optical flow is a **2D field in the image plane** describing apparent pixel motion between frames. Motion flow (sometimes used to mean *rigid-motion-induced image flow*) is the **image motion predicted by 3D scene structure and camera motion**. They coincide only under restrictive assumptions (brightness constancy, no occlusions, no non-rigid motion, no illumination change, etc.).

This post derives the core equations, shows where they break, and explains how these distinctions matter when you use flow for control, VO/VIO, and perception.

---

## Definitions

Let the image intensity be \( I(\mathbf{x}, t) \) with pixel coordinate \( \mathbf{x} = (x, y)^\top \).

**Optical flow** is a 2D vector field
\[
\mathbf{u}(\mathbf{x}) = (u(\mathbf{x}), v(\mathbf{x}))^\top
\]
such that a point at \( \mathbf{x} \) at time \( t \) moves to \( \mathbf{x} + \mathbf{u}(\mathbf{x}) \) at \( t + \Delta t \).

**Motion flow** is the flow field induced by camera rigid motion \((\mathbf{v}, \boldsymbol{\omega})\) and scene depth \(Z(\mathbf{x})\) through projective geometry. It is what you *should* observe in the image under a rigid world and calibrated camera.

Optical flow is *estimated from images* (photometric assumptions). Motion flow is *predicted from geometry* (kinematics + depth).

---

## Optical flow from brightness constancy

The classical starting point is **brightness constancy**:
\[
I(\mathbf{x}, t) = I(\mathbf{x} + \mathbf{u}(\mathbf{x}), t+\Delta t).
\]

For small motions, linearize:
\[
I(\mathbf{x} + \mathbf{u}, t+\Delta t) \approx I(\mathbf{x}, t) + I_x u + I_y v + I_t,
\]
giving the **optical flow constraint equation (OFCE)**:
\[
I_x u + I_y v + I_t = 0.
\]

This is one equation for two unknowns \((u,v)\) — the **aperture problem**. Any component of motion orthogonal to the local image gradient is unobservable:
\[
\mathbf{u} = \mathbf{u}_\parallel + \mathbf{u}_\perp,\quad \nabla I \cdot \mathbf{u}_\perp = 0.
\]

So, *optical flow is not “the” motion*; it’s the motion field that best satisfies photometric constraints plus whatever regularization you impose.

---

## Regularization: why every optical flow method encodes a prior

Most flow methods solve an optimization problem:

\[
\min_{\mathbf{u}} \int_\Omega \rho\!\left(I(\mathbf{x}+\mathbf{u}, t+\Delta t)-I(\mathbf{x}, t)\right)\,d\mathbf{x}
\;+\;
\lambda \int_\Omega \phi(\nabla \mathbf{u})\,d\mathbf{x}.
\]

- The first term enforces *data consistency* (brightness constancy, sometimes gradient constancy).
- The second enforces *spatial coherence* (smoothness, piecewise-smoothness, total variation, etc.).

This is not a nuisance detail: the choice of \(\phi\) is an *assumption about the world*.
In robotics scenes with thin structures, disocclusions, specularities, or rapid lighting change, many “accurate” benchmarks priors become liabilities.

---

## Motion flow from projective geometry

Now derive motion flow. Let a 3D point in camera coordinates be \(\mathbf{X}=(X,Y,Z)^\top\). Under a pinhole model (normalized image coordinates),
\[
x = \frac{X}{Z},\qquad y = \frac{Y}{Z}.
\]

Let the camera undergo instantaneous rigid motion: translational velocity \(\mathbf{v}=(v_x,v_y,v_z)^\top\) and angular velocity \(\boldsymbol{\omega}=(\omega_x,\omega_y,\omega_z)^\top\). The point’s velocity in camera coordinates (rigid scene) is:
\[
\dot{\mathbf{X}} = -\mathbf{v} - \boldsymbol{\omega}\times \mathbf{X}.
\]

Differentiate \(x = X/Z\), \(y = Y/Z\). The induced image velocity \((\dot{x},\dot{y})\) is:

\[
\begin{aligned}
\dot{x} &= \frac{1}{Z}\left(-v_x + x v_z\right) + x y\, \omega_x - (1+x^2)\,\omega_y + y\,\omega_z, \\
\dot{y} &= \frac{1}{Z}\left(-v_y + y v_z\right) + (1+y^2)\,\omega_x - x y\,\omega_y - x\,\omega_z.
\end{aligned}
\]

Key takeaways:

1. The translational component scales as \(1/Z\). Closer objects move faster in the image.
2. The rotational component is depth-independent (in normalized coordinates).
3. You need calibration (or work in pixel coordinates with intrinsics).

This is **motion flow**: a geometric prediction. If your estimated optical flow disagrees with this model, the disagreement is informative: non-rigidity, wrong depth, rolling shutter, photometric violation, or simply estimator bias.

---

## Relationship between optical flow and motion flow

When do they match?

They match if:
- brightness constancy holds (or your data term is correct),
- the scene is rigid (or motion model matches),
- depth is well-defined (no transparency, no reflections),
- there are no occlusions/disocclusions (or you model them),
- camera model matches reality (no rolling shutter, no unmodeled distortion).

In robotics, at least one of these is routinely violated. Treating optical flow as ground-truth motion is therefore incorrect.

---

## Failure modes that matter in robotics

### 1) Occlusion and disocclusion
Brightness constancy implicitly assumes correspondence exists. Occlusions violate that. Any estimator must either:
- detect occlusions and downweight data terms, or
- hallucinate motion that satisfies the regularizer.

Downstream effect: if you use flow for control (e.g., collision avoidance via time-to-contact), occlusion-induced artifacts can create false hazards or miss real ones.

### 2) Illumination and exposure changes
Outdoor robotics and SLAM systems see strong intensity variation. Classic fixes:
- gradient constancy,
- robust penalties \(\rho\),
- photometric calibration,
- learned features.

But note: once you move away from intensity constancy, “optical flow” becomes “flow in feature space.” That can still be useful, but interpret it accordingly.

### 3) Rolling shutter
Many robots use rolling-shutter cameras. The rigid-motion flow equation above assumes global shutter. Rolling shutter produces a time-varying pose during readout, which warps the flow field in systematic ways. If you ignore this, you will bias egomotion and depth estimates.

### 4) Dynamic scenes and non-rigid motion
If objects move independently, optical flow is a mixture of:
- rigid background motion flow (camera-induced),
- independent object motions,
- articulations.

This is why flow is often paired with segmentation or layered motion models in robotics perception stacks.

---

## Using flow for ego-motion: what is and isn’t identifiable?

From motion flow equations, translation is entangled with depth through \(1/Z\). This implies:

- With pure rotation (\(\mathbf{v}=0\)), you can estimate \(\boldsymbol{\omega}\) from flow without depth (in principle).
- With translation, you need depth (or additional constraints) to separate \(\mathbf{v}\) and \(Z\).

This is the root of **scale ambiguity** in monocular setups. You can get the *direction* of translation and the *structure up to scale* but not absolute scale without additional sensors or priors.

---

## Flow, depth, and time-to-contact

For planar motion towards a surface, an important quantity is **divergence** of flow (expansion), related to time-to-contact (TTC). In simplified cases,
\[
\text{TTC} \approx \frac{1}{\nabla \cdot \mathbf{u}}.
\]

But this is only meaningful if:
- the flow corresponds to actual motion (not regularizer artifacts),
- the scene is approximately rigid locally,
- you handle occlusions and independently moving objects.

This is why TTC-from-flow works well in constrained setups and can fail catastrophically in unconstrained environments.

---

## Practical guidance: what to do in robotics pipelines

### If your goal is VO/VIO robustness
- Use flow as a **measurement**, not truth.
- Prefer sparse, well-conditioned features (corners) if your estimator can’t robustly handle occlusions.
- If you use dense flow, explicitly model:
  - occlusion masks,
  - robust penalties,
  - rolling shutter if applicable,
  - camera intrinsics and distortion.

### If your goal is control (navigation, landing, avoidance)
- Avoid directly using raw dense flow magnitude as a proxy for distance.
- Use flow-derived quantities that are physically meaningful *under your assumptions*:
  - divergence (TTC),
  - focus of expansion (FOE) for heading,
  - epipolar constraints (rigid background).
- Gate estimates by confidence/occlusion and ignore regions dominated by independent motion.

### If your goal is dynamic scene understanding
- Combine flow with segmentation or multi-body motion models:
  - layered motion decomposition,
  - rigid background estimation + residual flow for objects,
  - scene flow (3D motion) if you have stereo/depth.

---

## Summary

- Optical flow is an **estimated 2D field** defined by photometric assumptions and regularization.
- Motion flow is the **physically predicted image velocity** induced by camera rigid motion and depth.
- Confusing the two leads to incorrect conclusions, especially in robotics where assumptions break routinely.
- The difference between them is not noise; it is often the signal (non-rigidity, occlusion, calibration errors, rolling shutter, illumination change).

---

## References

1. Horn, B. K. P., & Schunck, B. G. (1981). Determining optical flow.  
2. Lucas, B. D., & Kanade, T. (1981). An iterative image registration technique with an application to stereo vision.  
3. Longuet-Higgins, H. C., & Prazdny, K. (1980). The interpretation of a moving retinal image.  
4. Heeger, D. J., & Jepson, A. (1992). Subspace methods for recovering rigid motion I: Algorithm and implementation.
